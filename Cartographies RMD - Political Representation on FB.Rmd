---
author: "André K. Rodarte"
date: "2023-06-12"
output: html_document
---

# **Political Representation and Propaganda on Facebook: 2022 Brazilian Elections**

Nancy Fraser once described political representation as furnishing the stage where struggles for recognition and distribution occur. This stage, however, can be quite small. Brazil's democratic institutions have been historically criticized for failing to give voice to a range of social actors. The present study seeks to investigate whether Facebook reinforces, or subverts, this diagnosis.

Based on data about the 2022 election for Congress, I initially describe what kind of candidate did not campaign on that platform. The analyses indicate that most candidates who were absent from Facebook were "outsiders": candidates of color with lower levels of education, whose campaigns used significantly less party resources, and previously had no elected or state jobs.

My analysis turns next to candidates who did have an active account on Facebook. In exploring what campaigns rose to prominence on that platform, I show that already belonging to Brazil's political class and expenditure with Facebook promotion services predict greater likelihood of engagement on their posts. This finding reinforces, rather than subverts, the diagnosis of elite closure, as the unequal distribution of campaign resources help sustain the country's political class.

In addition to investigating inequalities in the "supply side," I also examine whether these candidates' campaigns were equitably distributed across their constituencies. Although Brazilian politicians claim to speak on behalf of vast regions of the country, my analysis demonstrate that certain municipalities are targeted much more than others, which may give rise to regions of representative scarcity in Brazil.

```{r setup, echo=T, results='hide', message=FALSE, warning=FALSE}
options(scipen = 999)  #R uses scientific notations as a default. 
#citation("DHARMa")
library(tidyverse)
library(readr)
library(stringdist)
library(datasets)
library(janitor)
library(lubridate)
library(tidytext)
library(stringr)
library(tokenizers)
library(wordcloud)
library(textdata)
library(geobr)
library(ggplot2)
library(sf)
library(dplyr)
library(ggmap)
library(broom)
library(RColorBrewer)
library(viridis)
library(igraph) 
library(ggraph) 
library(widyr)
library(janeaustenr)
library("lexiconPT")
library(stringi)
library(purrr)

#Regression analysis
#install.packages("caret")
#install.packages("car")
#install.packages("leaps")
#install.packages("MASS")
#install.packages("partykit")
#install.packages("nnet")
#install.packages("EMT")
#install.packages("DHARMa")
library(caret)
library(car)
library(leaps)
library(MASS)
library(partykit)
library(nnet)
library(EMT)
library(DHARMa)

#DATA VISUALIZATION
#install.packages('maps')
#install.packages("tibble")
#install.packages("forcats")
#install.packages("plotly")
library(maps)
library(plotly)
library(tibble)
library(forcats)

#Facebook Ads
library(highcharter)
library(httr)
library(furrr)
library(remotes)
```

```{r, message=FALSE, warning=FALSE}
#DATASETS
CrowdTangle <- readr::read_csv('(CART) (COLLECTTED 4-1-2023) CrowdTangle Historical-Report- Parliamentary Candidates MG - 2022-01-01-UNTIL-2023-01-02.csv') %>% 
  select(page_name, page_likes, page_followers, timestamp, body, body_description, views_total, interactions, comments, views_post, shares, likes, likes_wow, likes_love, likes_angry, likes_sad)
  
# Excluding posts created after the election.
CrowdTangle$timestamp <- as.Date(CrowdTangle$timestamp, format = "%m/%d/%Y")
CrowdTangle$postID<-row.names(CrowdTangle)
cutoff_date <- as.Date("2022-10-02")

# Subset the data frame to exclude posts before the cutoff date
CrowdTangle <- CrowdTangle[CrowdTangle$timestamp <= cutoff_date, ]
#This sample procedure yielded a total of 73,159 Facebook posts. 

#Data on the municipalities of the state of Minas Gerais (MG)
MGMunicipalities <- readr::read_csv('ListMunicipalitiesIBGE.csv',
    col_types = cols(Population = col_number(), 
        Medium_Salary = col_number(), PIB_per_capita = col_number(), 
        GINI = col_number(), Area = col_number(), 
        Urban_Area = col_number(), Demographic_Density = col_number()))

#Data on the campaign expenses 
dfExpensesTotal <- readr::read_csv("dfCampaignExpenses.csv")%>% 
  select(SG_UF, DS_CARGO, NR_CANDIDATO, full_name, party, party_abbrev, NM_FORNECEDOR, DS_ORIGEM_DESPESA, DS_DESPESA, VR_DESPESA_CONTRATADA) %>% 
  filter(DS_CARGO == "Deputado Federal")

  #Expenses grouped by candidate 
  dfExpenses <- dfExpensesTotal %>%
    group_by(full_name) %>%
    summarise(Expenses = sum(VR_DESPESA_CONTRATADA))
  #Adding expenses with online content 
  dfExpenses_FB <- dfExpensesTotal %>% 
          filter(str_detect(DS_ORIGEM_DESPESA, "Despesa com Impulsionamento de Conteúdos", negate = F)) %>% 
          group_by(full_name) %>%
          summarise(ExpensesAds = sum(VR_DESPESA_CONTRATADA))
  dfExpenses <- left_join(dfExpenses, dfExpenses_FB, by="full_name")
```

## CrowdTangle

I combined the TSE data set with information about the candidates' communications on social media, specifically their account names, number of views and number of interactions.

```{r, message=FALSE, warning=FALSE}
#First, I create a new dataset combining all interactions these candidates received during the election
CrowdTangle_Engagement <- CrowdTangle %>%
  group_by(page_name) %>%
  summarize(N_Posts = n(),
            Total_Views = sum(views_total),
            Total_Interactions = sum(interactions),
            Total_Comments = sum(comments),
            Total_Post_Views = sum(views_post),
            Total_Shares = sum(shares),
            Total_Likes = sum(likes),
            Total_Angy = sum(likes_angry))


#Had to combine manually this analysis with the previous TSE dataset.
write.csv2(CrowdTangle_Engagement, file = "CrowdTangle_Engagement.csv") 
#The new dataset is: 
dfTSE <- read_delim("df_TSE.csv", delim = ";", 
    escape_double = FALSE, trim_ws = TRUE)

df_combined <- left_join(dfTSE, CrowdTangle_Engagement, by="page_name")


#Translating the gender variables
df_combined$gender <- str_replace_all(df_combined$gender,'Masculino','Masculine')
df_combined$gender <- str_replace_all(df_combined$gender,'Feminino','Feminine')
#Translating the race variable
df_combined$race <- case_when(
  df_combined$race == 'BRANCA' ~ 'White',
  df_combined$race %in% c('PRETA', 'PARDA', 'AMARELA', 'INDÍGENA') ~ 'Non-White',
  df_combined$race == 'SEM INFORMAÇÃO' ~ 'No information',
  TRUE ~ df_combined$race
)
#Translating the education variable
df_combined$education <- case_when(
  df_combined$education == 'Superior completo' ~ 'College Complete',
  df_combined$education %in% c('Superior incompleto', 'Ensino Médio completo', 'Ensino Médio incompleto',
                               'Ensino Fundamental completo', 'Ensino Fundamental incompleto', 'Lê e escreve') ~ 'College incomplete or less',
  TRUE ~ df_combined$education
)
#Translating (some) previous occupations
df_combined$occupation <- case_when(
  df_combined$occupation == 'Gari ou Lixeiro' ~ 'Cleaner',
  df_combined$occupation == 'Deputado' ~ 'Deputy',
  df_combined$occupation == 'Ator e Diretor de Espetáculos Públicos' ~ 'Actor or Director of Public Spectacles',
  df_combined$occupation == 'Vereador' ~ 'Member of City Council',
  df_combined$occupation == 'Veterinário' ~ 'Veterinarian',
  df_combined$occupation == 'Servidor Público Federal' ~ 'Public Servant',
  df_combined$occupation == 'Policial Civil' ~ 'Police (Civil)',
  df_combined$occupation == 'Jornalista e Redator' ~ 'Journalist',
  df_combined$occupation == 'Advogado' ~ 'Lawyer',
  df_combined$occupation == 'Psicólogo' ~ 'Psychologist',
  df_combined$occupation == 'Fotógrafo e Assemelhados' ~ 'Photographer (and similar professions)',
  df_combined$occupation == 'Pecuarista' ~ 'Rancher',
  df_combined$occupation == 'Outros' ~ 'Other',
  df_combined$occupation == 'Empresário' ~ 'Businessperson',
  df_combined$occupation == 'Comerciante' ~ 'Merchant',
  df_combined$occupation == 'Administrador' ~ 'Manager',
  df_combined$occupation == "Aposentado (Exceto Servidor Público)" ~ 'Retired (except former public servants)',
  df_combined$occupation == 'Agente Administrativo' ~ 'Administrative Agent',
  df_combined$occupation == 'Agente de Saúde e Sanitarista' ~ 'Health and Sanitation Agent',
  TRUE ~ df_combined$occupation
)

#Parsing spent and received funds
df_combined$received_funds <- parse_number(df_combined$received_funds)
df_combined$spent_funds <- parse_number(df_combined$spent_funds)

#Creating a metric for absolute engagement
df_combined$Absolute_Engagement = rowSums(df_combined[,c("Total_Post_Views", "Total_Interactions", "Total_Comments")]) 

#Creating a metric for relative engagement
df_combined$Relative_Engagement = rowSums(df_combined[,c("Total_Post_Views", "Total_Interactions", "Total_Comments")]) / df_combined$N_Posts

#Adding data about candidate's declared wealth and campaign expenses
Wealth <- read.csv("Wealth.csv") %>% 
  select(full_name, Bens_Declarados)

colnames(Wealth)[colnames(Wealth) == "Bens_Declarados"] <- "Declared_Wealth"

df <- left_join(df_combined, Wealth, by="full_name")
df <- left_join(df, dfExpenses, by="full_name")
```

**Descriptive analysis of the entire sample**

```{r}
df_total <- df %>% 
  mutate(interaction = interaction(gender, race, education, sep = " ")) %>% 
  group_by(interaction) %>%
  summarise(Composition = n(),
            Expenses = mean(Expenses, na.rm = TRUE),
            ExpensesAds = mean(ExpensesAds, na.rm = TRUE),
            Wealth = mean(Declared_Wealth, na.rm = TRUE), 
            RelativeEngagement = mean(Relative_Engagement, na.rm = TRUE))
```

```{r}
df_total_arranged <- df_total %>%
  filter(!is.na(ExpensesAds)) %>%
  filter(!is.na(Wealth)) %>%
  filter(!is.na(RelativeEngagement))
  

df_total_arranged %>% 
  ggplot(aes(x=Expenses, y = fct_reorder(interaction, Expenses))) +
  geom_boxplot(fill = "slateblue", alpha = 0.2, notch = TRUE, fatten = 5,
               notchwidth = 1) +
  xlab("Campaign Expenditure") +
  ylab("")
```

```{r}
df_total_arranged %>% 
  ggplot(aes(x = RelativeEngagement, y = fct_reorder(interaction, RelativeEngagement))) +
  geom_boxplot(fill = "slateblue", alpha = 0.2, notch = TRUE, fatten = 5,
               notchwidth = 1) +
  xlab("Relative Engagement on Facebook") +
  ylab("")
```

# [RQ1] Supply side

## What kind of candidate was not on Facebook during the last electoral cycle?

```{r}
df_nas <- df[is.na(df$page_name), ] #Separating the dataset
```

```{r}
nas_intersections <- df_nas %>%
  mutate(interaction = interaction(gender, race, education, sep = " ")) %>% 
  group_by(interaction) %>%
  summarise(Composition = n(),
            Expenses = mean(Expenses, na.rm = TRUE),
            Wealth = mean(Declared_Wealth, na.rm = TRUE)) %>% 
  arrange(desc(Wealth),by_group = TRUE)

nas_intersections
```

**Analysis:** there were 1018 candidates who ran for the Chamber of Federal Deputies in 2022, 659 of them did not have an active, public account on Facebook. By analyzing the intersection of gender, race, and education, we can verify that the most numerous kind of candidate that was absent from this platform were men of color with lower levels of education (n = 133).

Specifically, we have:

|               |                           |                                |
|---------------|---------------------------|--------------------------------|
| **Gender**    | 256 women                 | 403 men                        |
| **Race**      | 361 non-white             | 291 white                      |
| **Education** | 287 with college complete | 372 college incomplete or less |

: *Table 1: Absent from Facebook*

After describing the composition of this group of candidates, I explored what resources they had to campaign. To verify what profile received less support from their political parties, I computed the mean value of campaign expenses per group. Consistent with the description above, this analysis revealed that candidates of color with lower levels of education ran on cheaper campaigns compared to other candidates. This finding suggests that campaign funding may be a determinant factor in the participation of candidates from minority backgrounds on Facebook.

It is important to note that there were fewer female candidates on Facebook, and compared to their male counterparts, they had less party resources --specifically, women without a college degree ranked the lowest in campaign expenses. Women of color already constituted the group with lowest declared wealth in this sample, so they could not resort to personal funding as much as other candidates.

Next, I was interested in identifying the most frequent previous occupation of this group. This analysis reveals that most candidates who were absent from Facebook came from careers other than politics.

```{r}
interactions_occupation <- df_nas %>%
     group_by(occupation) %>%
     summarise(Composition = n(), 
               Expenses = mean(Expenses, na.rm = TRUE)) 

interactions_occupation %>% 
  arrange(desc(Composition),by_group = TRUE)
```

### **Conclusion**

The analyses above indicate that most candidates who were absent from Facebook were "outsiders", that is, candidates who previously had no elected or state job. They were predominantly candidates of color with lower levels of education, whose campaigns used significantly less party resources. We can say that a typical case of a candidate who was not on Facebook was that of a non-white businessman who never graduated from college. This candidate's campaign expenses was less than half the average of the entire sample.

A significant proportion of female candidates did not have an active Facebook account (256 of a total of 343). This group ran their campaigns on lower budget and they could not resort to the same level of personal wealth as their male counterparts. Among those without a Facebook account, it is noteworthy that a woman of color without a college degree spent on average only fifty eight thousand on her campaign. For the sake of comparison, a white male candidate with higher education who was on Facebook spent eleven times more.

## **Is there a relationship between engagement on Facebook and candidates' profiles?**

My literature review suggested that candidates would not compete on equal footing over their constituencies of interest. Traditional politicians ---predominantly white, upper class, men--- have historically received greater attention in the media. So I hypothesized that social media platforms would extend their visibility in ways that still reproduce inequalities in access to means of communication, conforming to what Strandberg (2013) described as normalization thesis.

To verify that, I conducted the following analyzes: first, I counted the number of candidates in each demographic group and combined the number of interactions each group received during this election.

```{r}
df_clean <- df %>% 
  filter(!is.na(N_Posts))

table(df_clean$education) #A similar code was used to verify the number of candidates of different gender and race. 
```

|               |                                |                      |
|---------------|--------------------------------|----------------------|
| **Gender**    | 86 women                       | 265 men              |
| **Race**      | 146 non-white                  | 205 white            |
| **Education** | 107 college incomplete or less | 244 college complete |

: *Table 1: Candidates with a Facebook account*

### Voters interacted the most with candidates of what gender?

```{r}
interactions_gender <- df_clean %>%
     group_by(gender) %>%
     summarise(n = n(), Engagement = sum(Absolute_Engagement), Mean = mean(Absolute_Engagement), sd = sd(Absolute_Engagement), Comments = sum(Total_Comments), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE)) 

interactions_gender %>% 
  arrange(desc(Mean),by_group = TRUE)

```

### Voters interacted the most with candidates of what race?

```{r}
interactions_race <- df_clean %>%
     group_by(race) %>%
     summarise(n = n(), Engagement = sum(Absolute_Engagement), Mean = mean(Absolute_Engagement), sd = sd(Absolute_Engagement), Comments = sum(Total_Comments), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE)) 
    

interactions_race %>% 
  arrange(desc(Mean),by_group = TRUE)
```

### Voters interacted the most with candidates of what level of education?

```{r}
interactions_education <- df_clean %>%
     group_by(education) %>%
     summarise(n = n(), Engagement = sum(Absolute_Engagement), Mean = mean(Absolute_Engagement), sd = sd(Absolute_Engagement), Comments = sum(Total_Comments), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE)) 

interactions_education %>% 
  arrange(desc(Mean),by_group = TRUE)
```

### Intersectional analysis: voters interacted the most with what profile of candidate?

```{r, message=FALSE, warning=FALSE}
#Adding political class
df_clean <- df_clean %>% 
  mutate(PoliticalClass = df_clean$occupation) 
df_clean$PoliticalClass <- case_when(
    df_clean$PoliticalClass %in% c('Deputy', 'Member of City Council') ~ '1', TRUE ~ '0')

# Create the interaction term by combining the categorical variables
df_interaction <- df_clean %>% 
  #mutate(interactionRG = interaction(race, gender, sep = " ")) %>% 
  #mutate(interactionGE = interaction(gender, education, sep = " ")) %>% 
  mutate(interaction = interaction(gender, race, education, sep = " "))

intersectional_analysis <- df_interaction %>%
     group_by(interaction) %>%
     summarise(n = n(), Engagement_Total = sum(Absolute_Engagement), Engagement_Mean = mean(Absolute_Engagement), sd = sd(Absolute_Engagement), Comments = sum(Total_Comments), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE)) 
    
intersectional_analysis %>% 
  arrange(desc(Engagement_Mean),by_group = TRUE)
```

### Voters interacted the most with candidates of what previous occupation?

```{r}
occupation_interactions <- df_clean %>%
     group_by(occupation) %>%
     summarise(n = n(), Engagement = sum(Absolute_Engagement), Mean = mean(Absolute_Engagement), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE)) 

occupation_interactions %>% 
  arrange(desc(Mean),by_group = TRUE)
```

In absolute numbers, most interactions that occurred on Facebook were unequally distributed: they were directed to white, male candidates with higher levels of education. Considering the low number of female candidates on this platform (86, only ten of which were women of color without a college degree), it is not surprising that we found this distribution in absolute numbers. Yet, with the exception of education, we found a similar ranking when calculating the average engagement per groups.

This analysis is limited insofar as the sample composition is highly skewed. Nevertheless, I am reporting these results because, even if these findings are questionable on a statistical level, they are revealing on a political one.

If we consider the intersection of our variables of interest, the groups that received the most interactions online were composed of man of color without college degree (even though their campaigns ran on relatively low budget), followed by white man of all levels of education.

Parliamentarians seeking re-election and former members of municipal councils ranked high in this analysis, which corroborates with previous studies suggesting that Brazilian elections for Congress favor incumbents and those already belonging to the country's political class. (The other previous occupations that figured on top were cleaner, actor, and photographer, but there were only three or less candidates from each of these occupations.)

### Voters interacted the most with candidates of what political party?

```{r}
interactions_party <- df_clean %>%
     group_by(party_abbrev) %>%
     summarise(n = n(), Engagement = sum(Absolute_Engagement), Mean = mean(Absolute_Engagement), Comments = sum(Total_Comments), Expenses = mean(Expenses, na.rm = T), ExpensesAds = mean(ExpensesAds, na.rm = TRUE))

interactions_party %>% 
  arrange(desc(Mean),by_group = TRUE)
```

### Do these variables explain what candidates received more interactions online?

My next step was to examine whether gender, race, education helped explain what candidates received more interactions online. To that end, I included information about their declared assets (wealth) and about their campaign budget, specifically: the amount of money they received to campaign, the amount of money they declared to have used, and the amount of money they declared to have spent with Facebook services.

```{r}
CorrelationFundsEngagement <- cor.test(df_clean$ExpensesAds, df_clean$Relative_Engagement, 
                    method = "pearson")

CorrelationFundsEngagement
```

```{r}
BV_Engagement <- ggplot()+
  geom_jitter(data=df_clean, aes(x = ExpensesAds, y = Relative_Engagement, color = party_abbrev, text=name_ballot)) + 
  labs(title = "Relationship between online campaign expenses and engagement", x = "Expenses", y = "Engagement online")+
  guides(col = guide_legend(title = "", ncol = 1))
  
InteractiveEngagement <- ggplotly(BV_Engagement)

InteractiveEngagement
```

A Pearson's correlation test suggested a weak positive relation between campaign expenses and engagement online, but this correlation was not significant. Considering how skewed the sample is, and the number of categorical variables in it, I conducted a negative binominal regression to further investigate the relationship between those variables.

#### **Z-Transformed Regression Analysis**

Over-dispersal of count data can be verified in our dependent variable if the variance exceeds the means. This is the case with "Relative Engagement".

```{r}
mean(df_clean$Relative_Engagement)
var(df_clean$Relative_Engagement)
```

Because my dependent variable, "relative engagement," is a count with an over-dispersed distribution, the most appropriate statistical test is a negative binomial regression.

```{r}
#Transforming the data
df_ztransformed <- df_clean

#Adding political class
df_ztransformed <- df_ztransformed %>% 
  mutate(PoliticalClass = df_ztransformed$occupation) 
df_ztransformed$PoliticalClass <- case_when(
    df_ztransformed$PoliticalClass %in% c('Deputy', 'Member of City Council') ~ '1', TRUE ~ '0')
df_ztransformed$PoliticalClass <- as.factor(df_ztransformed$PoliticalClass)

#Adding number of ads
df_ztransformed_preliminary <- df_ads_combined[, c("nr_ads", "page_name")]

     df_ztransformed_preliminary$page_name = stri_trans_general(str =                     df_ztransformed_preliminary$page_name, id = "Latin-ASCII")
     df_ztransformed_preliminary$page_name <-                                             tolower(df_ztransformed_preliminary$page_name)

     df_ztransformed$page_name = stri_trans_general(str = df_ztransformed$page_name,      id = "Latin-ASCII")
     df_ztransformed$page_name <- tolower(df_ztransformed$page_name)

df_ztransformed <- merge(df_ztransformed, df_ztransformed_preliminary, by = "page_name")
```

```{r}
#Dealing with missing values
#Removing NAs
df_ztransformed$nr_ads[is.na(df_ztransformed$nr_ads)] <- 0
df_ztransformed$ExpensesAds[is.na(df_ztransformed$ExpensesAds)] <- 0

#Wealth | In this case, I want to make sure that the imputation will maintain a similar mean, sd, median and distribution. 
mean(df_ztransformed$Declared_Wealth, na.rm = T)
sd(df_ztransformed$Declared_Wealth, na.rm = T)
median(df_ztransformed$Declared_Wealth, na.rm = T)
```

```{r}
#https://appsilon.com/imputation-in-r/
#install.packages("mice")
library(mice)

df_ztransformed_preliminary <- df_ztransformed %>%
  select(gender, race, education, occupation, PoliticalClass, received_funds, Declared_Wealth)
md.pattern(df_ztransformed_preliminary)
```

```{r}
df_ztransformed_imputed <- data.frame(
  Declared_Wealth = df_ztransformed$Declared_Wealth,
  Wealth_pmm = complete(mice(df_ztransformed_preliminary, method = "pmm"))$Declared_Wealth,
  Wealth_cart = complete(mice(df_ztransformed_preliminary, method = "cart"))$Declared_Wealth)

df_ztransformed_imputed
```

```{r}
#pmm: Predictive mean matching 
mean(df_ztransformed_imputed$Wealth_pmm)
sd(df_ztransformed_imputed$Wealth_pmm)
median(df_ztransformed_imputed$Wealth_pmm)
```

```{r}
#cart: Classification and regression trees
mean(df_ztransformed_imputed$Wealth_cart)
sd(df_ztransformed_imputed$Wealth_cart)
median(df_ztransformed_imputed$Wealth_cart)
```

Result: cart proved to be a better imputation method.

```{r}
df_ztransformed <-cbind(df_ztransformed, df_ztransformed_imputed)
```

```{r}
#Expenses | I want to make sure that the imputation will maintain a similar mean, sd, median and distribution. 
mean(df_ztransformed$Expenses, na.rm = T)
sd(df_ztransformed$Expenses, na.rm = T)
median(df_ztransformed$Expenses, na.rm = T)
```

```{r}
df_ztransformed_preliminary2 <- df_ztransformed %>%
    select(gender, race, education, PoliticalClass, Wealth_cart, Expenses)
  md.pattern(df_ztransformed_preliminary2)

df_ztransformed_imputed2 <- data.frame(
  Expenses = df_ztransformed$Expenses,
  Expenses_cart = complete(mice(df_ztransformed_preliminary2, method = "cart"))$Expenses)
```

```{r}
mean(df_ztransformed_imputed2$Expenses_cart)
sd(df_ztransformed_imputed2$Expenses_cart)
median(df_ztransformed_imputed2$Expenses_cart)
```

```{r}
df_ztransformed <-cbind(df_ztransformed, df_ztransformed_imputed2)
```

```{r}
#Transforming categorical into interger 
df_ztransformed$gender <- str_replace_all(df_ztransformed$gender,
                                            'Feminine','0')
df_ztransformed$gender <- str_replace_all(df_ztransformed$gender,
                                            'Masculine','1')
df_ztransformed$gender <- as.factor(df_ztransformed$gender)

df_ztransformed$race <- str_replace_all(df_ztransformed$race,
                                          "Non-White", "0")
df_ztransformed$race <- str_replace_all(df_ztransformed$race,
                                          "White", "1")
df_ztransformed$race <- as.factor(df_ztransformed$race)

df_ztransformed$education <- str_replace_all(df_ztransformed$education,
                                            'College incomplete or less','0')
df_ztransformed$education <- str_replace_all(df_ztransformed$education,
                                            'College Complete','1')
df_ztransformed$education <- as.factor(df_ztransformed$education)

#Z Transformation: Subtracting the mean from all values of an observation and dividing the observation by the standard deviation of all values 
df_ztransformed$Expenses_Z <- scale(df_ztransformed$Expenses_cart)
df_ztransformed$ExpensesAds_Z <- scale(df_ztransformed$ExpensesAds)
df_ztransformed$ReceivedFunds_Z <- scale(df_ztransformed$received_funds)
df_ztransformed$Declared_Wealth_Z <- scale(df_ztransformed$Wealth_cart)

# Round the values to integers
df_ztransformed$Relative_Engagement <- round(df_ztransformed$Relative_Engagement)
```

#####Negative binomial regression model #####First iteration

```{r}
# Controling for demographic variables
Model_Z1 <- glm.nb(Relative_Engagement ~ 
                      gender + race + education + Declared_Wealth_Z, 
                 data = df_ztransformed,
                 maxit = 1000) # I increased the maximum number of iterations for the optimization algorithm by setting the maxit parameter in the glm.nb function.
```

```{r}
# Check for multicollinearity
vif_results_Z1 <- vif(Model_Z1) %>% 
  round(2)
print(vif_results_Z1)
```

```{r}
# Print the summary of the model
summary(Model_Z1)
#To export the results, use tidy and then create a data frame 
#tidymodel <- tidy(Model_Z1)
#write.csv2(tidymodel, file = "tidymodel.csv", row.names=FALSE)
```

#####Second iteration

```{r}
# Negative binomial regression model
Model_Z2 <- glm.nb(Relative_Engagement ~ 
                      gender + race + education + Declared_Wealth_Z
                    + PoliticalClass + ReceivedFunds_Z + 
                    + ExpensesAds_Z + Expenses_Z, 
                 data = df_ztransformed,
                 maxit = 1000) # I increased the maximum number of iterations for the optimization algorithm by setting the maxit parameter in the glm.nb function.
```

```{r}
# Check for multicollinearity
vif_results_Z2 <- vif(Model_Z2) %>% 
  round(2)
print(vif_results_Z2)
```

```{r}
# Print the summary of the model
summary(Model_Z2)
#To export the results, use tidy and then create a data frame 
#tidymodel2 <- tidy(Model_Z2)
#write.csv2(tidymodel2, file = "tidymodel2.csv", row.names=FALSE)
```

```{r}
nas_regression <- colSums(is.na(df_ztransformed))
  print(nas_regression)
#Note that 35 candidates did not provide information about their campaign expenses.
```

```{r}
#Gina suggested extracting coefficient estimates for the highly significant results
#Formula to calculate the IRR for "Expenses":
#IRR = exp(coefficient estimate)
IRR_gender = exp(0.64347)
IRR_ExpensesAds = exp(0.74240)
IRR_ReceivedFunds = exp(0.37015)
IRR_PoliticalClass = exp(1.30468)
```

### **Residual diagnosis for the negative binomial test**

```{r}
ResidualsOutput <- simulateResiduals(fittedModel = Model_Z2, plot = F)
#residuals(ResidualsOutput)
plot(ResidualsOutput)
```

**Kolmogorov-Smirnov test: p-value = 0.00046** **\|** The KS test checks if the distribution of the simulated residuals generated by DHARMa significantly differs from the expected distribution under the model. This indicates that the model might not fit the data well in terms of overall distribution.

**Dispersion test: p-value =** 0.792 **\|** The dispersion test assesses if there is a significant deviation in the dispersion of the residuals compared to what the model assumes. A high p-value (0.736) means that there is no significant deviation in the dispersion of residuals. This is a good sign, indicating that the model is doing well in capturing the variability in the data.

**Outlier test: p-value = 0.1 \|** The outlier test checks if there are significant outliers in the data, which can be problematic for the model. A p-value of zero means that there are significant outliers present in the data, and they deviate from what the model expects.

**Summary:** results indicate that the dispersion of the residuals is not significantly deviating from what the model assumes, and the outlier test also had non significant results. However, they indicate that there are significant deviations in terms of the overall distribution (KS test). Additionally, quantile deviations suggest that the model might not be accurately capturing certain patterns in the data at specific points along the distribution.

### Conclusion:

**Reporting results from model 1 \|** [TO BE REWRITTEN]

The negative binomial regression showed that already belonging to Brazil's political class (β=1.50, IRR = ≈ 1.16, p \<0.01), campaign expenses (β=1.10, IRR = ≈ 1.00, p \<0.001) and, more specifically, expenses with Facebook services (β=8.98, IRR = ≈ 1.00, p \<0.001) had a significant relationship with online engagement.

Based on this statistical test, we can conclude that online engagement proved to have a strong relationship with already belonging to Brazil's political class, and an even more significant relationship with campaign and Facebook expenses. These results indicate that Facebook does not level the playing field for candidates, as being an incumbent and having enough economic resources are determinant factors for a candidacy's visibility on that platform.

# [RQ2] Distribution

***What are the constituencies (geographically defined) parliamentarians reached out to the most? Conversely, what constituencies were left aside in this election?***

Brazilian parliamentarians often claim that their elections give them legitimacy to speak on behalf of the people. This legitimacy discourse is premised on the notion that parliamentarians campaign across their states, going so far as to reach their most secluded regions.

In order to test that, I investigated all candidates' political communication in search of references to the municipalities they campaigned for. I asked whether there were constituencies which received much less attention during this electoral period, thus being left aside in these campaigners' political communication.

My first step was to count the number of times any given municipality in the state of Minas Gerais was mentioned in the posts collected from CrowdTangle.

```{r, echo=T, results='hide', message=FALSE, warning=FALSE}
# All municipalities in the state of Minas Gerais
muni <- read_municipality(code_muni= "MG", 
                          year=2020)

################################
####Transforming Latin/Portuguese characters
## CrowdTangle
CrowdTangle_Clean <- CrowdTangle 
CrowdTangle_Clean$body = stri_trans_general(str = CrowdTangle_Clean$body, id = "Latin-ASCII")                                             #removing accents
CrowdTangle_Clean$body <- tolower(CrowdTangle_Clean$body)  #lower case

##Municipality names
muni_clean <- muni
muni_clean$name_muni = stri_trans_general(str = muni_clean$name_muni, id = "Latin-ASCII")   
muni_clean$name_muni <- tolower(muni_clean$name_muni)

################################
#Tokenization and identification
#First, we need to check whether each element in the body variable contains any of the municipality names.
posts_with_muni <- CrowdTangle_Clean %>% 
          filter(
          str_detect(body, paste(muni_clean$name_muni, collapse = "|"), negate = F)
          )   #The paste() function concatenates the names into a single string, with the collapse argument set to | to create a regular expression pattern that matches any of the names. 
              #The str_detect() function returns a logical vector indicating whether each element in body matches the pattern. The negate = F argument ensures that the matches are not negated (i.e., it returns TRUE for matching elements).

posts_with_muni_clean <- posts_with_muni
posts_with_muni_clean$body = stri_trans_general(str = posts_with_muni_clean$body, id = "Latin-ASCII")   
posts_with_muni_clean$body <- tolower(posts_with_muni_clean$body)

```

**Result:** this procedure identified 29,536 posts that mentions municipalities from Minas Gerais.

```{r, results='hide', message=FALSE, warning=FALSE}
#Now, I will identify the occurences in the corpus (both across the dataset and per candidate). To that end, the first step will be to extract all occurences of a pattern from a string.
muni2 <- str_extract_all(
  CrowdTangle_Clean$body, 
  paste(muni_clean$name_muni, collapse = "|"), #Paste() function concatenates all elements from the muni_clean$name_muni vector into a single string. The collapse argument is set to | to create a regular expression pattern that matches any of the names. 
  simplify = FALSE)

muni_list <- muni2 %>% 
  unlist() %>%  #unlist produces a vector which contains all the atomic components which occur in a column. 
  as.data.frame()
  colnames(muni_list) <- "name_muni"  #pulling out the names of the columns                                              (changing . for something meaningful)
muni_mentioned <- muni_list %>%  
  group_by(name_muni) %>% 
  count()

#Adding the number of mentions to the datalist 
muni_clean <- as.data.frame(muni_clean) #because R considered it a "large list", not a dataframe.
muni3 <- left_join(muni_clean, muni_mentioned, by = "name_muni", copy = FALSE, keep = FALSE)

muni3$freq <- muni3$freq %>% replace_na(0)

####################################################
########  REPLACE AMBIGUOUS NAMES
muni3 <- muni3 %>%
  mutate(n = ifelse(name_muni == "bandeira", 28, n))  %>% 
  mutate(n = ifelse(name_muni == "conquista", 12, n)) %>% 
  mutate(n = ifelse(name_muni == "divino", 65, n))    %>% 
  mutate(n = ifelse(name_muni == "extrema", 22, n))   %>%
  mutate(n = ifelse(name_muni == "liberdade", 9, n))  %>%
  mutate(n = ifelse(name_muni == "luz", 5, n))        %>% 
  mutate(n = ifelse(name_muni == "machado", 43, n))   %>% 
  mutate(n = ifelse(name_muni == "passos", 103, n))   %>% 
  mutate(n = ifelse(name_muni == "paiva", 1, n))      %>% 
  mutate(n = ifelse(name_muni == "campanha", 4, n))   %>% 
  mutate(n = ifelse(name_muni == "cristina", 0, n))   %>% 
  mutate(n = ifelse(name_muni == "goncalves", 0, n))  %>% 
  mutate(n = ifelse(name_muni == "pimenta", 15, n))
```

```{r, results='hide', message=FALSE, warning=FALSE}
#UNIQUE MENTIONS PER CANDIDATE
#Exclude ambiguous names 
ambiguous_names <- c("campanha", "cristina", "bandeira", "conquista", "divino", "extrema", "goncalves", "liberdade", "luz", "machado", "passos", "paiva", "pimenta")
filtered_muni_names <- muni_clean$name_muni[!(muni_clean$name_muni %in% ambiguous_names)]

###############################
# Count mentions of filtered_muni_names in the body column
muni_mentions_analysis <-  posts_with_muni %>%
  mutate(mentioned_cities = str_extract_all(body, paste(filtered_muni_names, collapse = "|"))) %>%
  unnest(mentioned_cities) %>%
  group_by(page_name, mentioned_cities) %>%
  summarise(
    total_mentions = n(),
    unique_mentions = 1,
    cities_mentioned = paste(mentioned_cities, collapse = ", ")
  ) %>%
  group_by(page_name) %>%
  summarise(
    total_mentions = sum(total_mentions),        #Total count of city mentions for each candidate
    unique_mentions = n(),                       #Number of unique cities mentioned for each candidate
    cities_mentioned = list(unique(mentioned_cities))   #A list of city names mentioned by each candidate
  )

###############################
#### Adding ambiguous names ###
###############################
muni_mentions_analysis_combined <- full_join(muni_mentions_analysis, muni_ambiguous_analysis, by = c("page_name" = "page_name.y")) %>%
  mutate(
    total_mentions = ifelse(is.na(total_mentions.y), total_mentions, total_mentions + total_mentions.y),
    unique_mentions = ifelse(is.na(unique_mentions.y), unique_mentions, unique_mentions + unique_mentions.y),
    cities_mentioned = map2(cities_mentioned, cities_mentioned.y, union)
  ) %>%
  select(page_name, total_mentions, unique_mentions, cities_mentioned)
```

**What is the average number of unique city mentions by a candidate?**

```{r}
mean(muni_mentions_analysis_combined$unique_mentions, na.rm = TRUE)
median(muni_mentions_analysis_combined$unique_mentions, na.rm = TRUE)
sd(muni_mentions_analysis_combined$unique_mentions, na.rm = TRUE)
```

**What is the average number of city mentions by a candidate?**

```{r}
mean(muni_mentions_analysis_combined$total_mentions, na.rm = TRUE)
median(muni_mentions_analysis_combined$total_mentions, na.rm = TRUE)
sd(muni_mentions_analysis_combined$total_mentions, na.rm = TRUE)
```

**Adding Census Data**

```{r}
MGMunicipalities_Clean <- MGMunicipalities
MGMunicipalities_Clean$name_muni = stri_trans_general(str = MGMunicipalities_Clean$name_muni, id = "Latin-ASCII")   #removing accents
MGMunicipalities_Clean$name_muni <- tolower(MGMunicipalities_Clean$name_muni) #lower case

IBGE <- left_join(muni3, MGMunicipalities_Clean, by = "name_muni", copy = T, keep = FALSE)
IBGE <- replace(IBGE, is.na(IBGE), 0)
```

**How many municipalities were mentioned less than ten times?**

```{r}
sum(IBGE$n <10, na.rm=TRUE)
```

# Correlations

```{r}
CorrelationPopulation <- cor.test(IBGE$n, IBGE$Population, 
                    method = "pearson")
#Correlation between mentions and Human Development Index (IDHM, in Portuguese)
CorrelationGINI <- cor.test(IBGE$n, IBGE$GINI,method = "pearson")
CorrelationPIB <- cor.test(IBGE$n, IBGE$PIB_per_capita, method = "pearson")


CorrelationPopulation
CorrelationGINI
CorrelationPIB
```

```{r}
ggplot(data = IBGE) + 
  geom_jitter() +
  aes(x = n, y = GINI, color = Mesoregion)+
  labs(title = "Relationship between mentions and Human Development Index", x = "Mentions", y = "Human Development Index")
```

```{r}
ggplot(data = IBGE) + 
  geom_jitter() +
  aes(x = n, y = Population, color = Mesoregion)+
  labs(title = "Relationship between mentions and population", x = "Mentions", y = "Population")
```

```{r}
ggplot(data = IBGE) + 
  geom_jitter() +
  aes(x = n, y = PIB_per_capita, color = Mesoregion)+
  labs(title = "Relationship between mentions and PIB", x = "Mentions", y = "PIB per capita")
```

```{r, message=FALSE, warning=FALSE}
BV <- ggplot()+
  geom_jitter(data=IBGE, aes(x = n, y = Population, text=name_muni)) + 
  labs(title = "Relationship between mentions and population (estimated for 2021)", x = "Mentions", y = "population")+
  guides(fill = guide_colourbar(title = "Mentions")) 
  
InteractivePopulation <- ggplotly(BV)
InteractivePopulation
```

```{r}
BV + facet_wrap(vars(Mesoregion))
```

```{r}
ggplot(data = IBGE) + 
  geom_jitter() +
  aes(x = n, y = PIB_per_capita, color = Mesoregion)+
  labs(title = "Relationship between PIB per capita and mentions", x = "Mentions", y = "PIB per capita")
```

## **Regression**

```{r}
hist(IBGE$Medium_Salary, 
     breaks = 100,  # Choose the number of bins automatically using the 'FD' method
     col = "skyblue",  # Set the color of the bars
     border = "white",  # Set the color of the bar borders
     ylab = "",
     xlab = "",  # Set the x-axis label
     main = "Histogram of Population"  # Set the main title
)
```

```{r}
IBGE_log <- IBGE

#Log transformation
IBGE_log$Population <- log(IBGE_log$Population + 1)
IBGE_log$PIB_per_capita <- log(IBGE_log$PIB_per_capita + 1)
IBGE_log$Urban_Area <- log(IBGE_log$Urban_Area + 1)
IBGE_log$Demographic_Density <- log(IBGE_log$Demographic_Density + 1)


hist(IBGE_log$Population, 
     col = "skyblue",  # Set the color of the bars
     border = "white",  # Set the color of the bar borders
     xlab = "",  # Set the x-axis label
     main = "Histogram of Population"  # Set the main title
)

```

```{r}
Model_Geo <- lm(n ~ Population + GINI + Medium_Salary 
                       + Urban_Area + Demographic_Density,
                       data = IBGE_log)
```

```{r}
omnibus_geo <- Anova(Model_Geo, type = "III")
omnibus_geo
```

```{r}
# Check for multicollinearity using vif()
vif_geo <- vif(Model_Geo)
print(vif_geo)
```

```{r}
summary(Model_Geo)
#To export the results, use tidy and then create a data frame 
tidymodel_geo <- tidy(Model_Geo)
```

## Geographical analysis

Based on the number of times municipalities *mineiras* were mentioned, I plotted an interactive choropleth map. The map allows the reader to hoover over the municipalities and see their names and the number of times they were mentioned.

```{r, message=FALSE, warning=FALSE}
# Ploting an interactive choropleth map
Map <- ggplot() +
  geom_sf(data=IBGE, aes(fill=n, text = paste(name_muni, "population:", Population), geometry = geom), color= NA, size=.15) + 
    labs(title="2022 Election: municipalities mentioned", size=10) +
    guides(fill = guide_colourbar(title = "Mentions")) +
    theme_minimal() +
    theme(axis.line = element_blank(), axis.text = element_blank(),
        axis.ticks = element_blank(), axis.title = element_blank(),
        panel.grid = element_blank()) +
    scale_fill_viridis(limits = c(0, 2200))

InteractiveMap <- ggplotly(Map) %>% 
  layout(title = list(text = paste0("<B>2022 Election: municipalities mentioned</B>")))

InteractiveMap
```

**Summary of findings:** There were 29,536 posts that made reference to municipalities *Mineiras* during the last election. The analysis reveals that candidates, on average, make reference to 30 municipalities. However this average is highly skewed: the median is 16 municipalities, and the standard deviation is 38. Moreover, the results indicate that, on average, candidates mentioned municipalities 159 times throughout the electoral cycle. These results were also highly skewed: the median was 56 and the standard deviation was 262.

I suggested that, despite our political elite's avowed goals to scatter in the corners of the nation, perhaps they persistently neglect certain constituencies in benefit of others. Indeed, I found that, in a state of 853 municipalities, 30 of them were mentioned one hundred times or more by candidates. And in a stark demonstration that electoral campaigns do not circulate as far as previously claimed, more than **260 municipalities were mentioned less than ten times**. Being from a populous city, with high GINI coefficient, significantly increased the chances of having it mentioned in their campaigns.
